spring:
  main:
    web-application-type: none
    lazy-initialization: true # ‚ö° Speed up startup by creating beans only when needed
  ai:
    ollama:
      base-url: http://127.0.0.1:11434
      chat:
        options:
          model: qwen2.5-coder:14b
          temperature: 0.3
      embedding:
        options:
          model: nomic-embed-text:latest
      keep-alive: -1

llama:
  default-provider: gemini
  providers:
    - name: gemini
      type: gemini
      settings:
        fallbacks: "gemini-2.5-flash,auto,gemini-3-pro-preview"
    - name: ollama
      type: ollama
      settings:
        base-url: "http://localhost:11434"
        model: "llama3"
    - name: codex
      type: codex
      settings:
        command: "codex-cli"
    - name: opencode
      type: opencode
      settings:
        endpoint: "http://opencode-ai.local"

# üïµÔ∏è‚Äç‚ôÇÔ∏è FULL TRACE LOGGING: No more hidden failures
logging:
  level:
    root: INFO
    org.springframework.ai.ollama: INFO
    org.springframework.ai.chat.model: INFO
    org.springframework.web.client.RestClient: INFO
    org.springframework.http.client: INFO
    org.springframework.web.reactive.function.client.WebClient: INFO
    reactor.netty.http.client: INFO
    com.example.llama.application.orchestrator: DEBUG
