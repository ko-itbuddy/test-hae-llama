spring:
  main:
    web-application-type: none
    lazy-initialization: true # ‚ö° Speed up startup by creating beans only when needed
  ai:
    ollama:
      base-url: ${LLAMA_OLLAMA_BASE_URL:https://api.ollama.cloud} # Placeholder for a cloud endpoint
      chat:
        options:
          model: ${LLAMA_OLLAMA_MODEL:qwen3-coder:480b-cloud}
          temperature: 0.0
      keep-alive: -1

llama:
  default-provider: gemini
  providers:
    - name: gemini
      type: gemini
      settings:
        fallbacks: "gemini-2.0-flash,gemini-1.5-pro"
    - name: ollama
      type: ollama
      settings:
        base-url: ${LLAMA_OLLAMA_BASE_URL:https://api.ollama.cloud}
        model: ${LLAMA_OLLAMA_MODEL:qwen3-coder:480b-cloud}
    - name: codex
      type: codex
      settings:
        command: "codex-cli"
    - name: opencode
      type: opencode
      settings:
        endpoint: "http://opencode-ai.local"

# üïµÔ∏è‚Äç‚ôÇÔ∏è FULL TRACE LOGGING: No more hidden failures
logging:
  level:
    root: INFO
    org.springframework.ai.ollama: INFO
    org.springframework.ai.chat.model: INFO
    org.springframework.web.client.RestClient: INFO
    org.springframework.http.client: INFO
    org.springframework.web.reactive.function.client.WebClient: INFO
    reactor.netty.http.client: INFO
    com.example.llama.application.orchestrator: DEBUG
